{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "py36"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    },
    "colab": {
      "name": "Copy of HW-1 Part 1. Spam Prediction.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p7Z8eeN5IW9q",
        "colab_type": "text"
      },
      "source": [
        "# Part 1.\n",
        "\n",
        "The deadline for Part 1 is **1:30 pm Feb 6th, 2020**.   \n",
        "You should submit a `.ipynb` file with your solutions to NYU Classes.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "In this part we will preprocess SMS Spam Collection Dataset and train a bag-of-words classifier (logistic regression) for spam detection. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dZd0LJzbISPd",
        "colab_type": "text"
      },
      "source": [
        "## Data Loading\n",
        "\n",
        "First, we download the SMS Spam Collection Dataset. The dataset is taken from [Kaggle](https://www.kaggle.com/uciml/sms-spam-collection-dataset/data#) and loaded to [Google Drive](https://drive.google.com/open?id=1OVRo37agn02mc6yp5p6-wtJ8Hyb-YMXR) so that everyone can access it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PvGErs2oHkWU",
        "colab_type": "code",
        "outputId": "33f74074-6217-4a91-81a2-27e929da6e39",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        }
      },
      "source": [
        "!wget 'https://docs.google.com/uc?export=download&id=1OVRo37agn02mc6yp5p6-wtJ8Hyb-YMXR' -O spam.csv "
      ],
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-02-12 18:08:44--  https://docs.google.com/uc?export=download&id=1OVRo37agn02mc6yp5p6-wtJ8Hyb-YMXR\n",
            "Resolving docs.google.com (docs.google.com)... 172.217.193.138, 172.217.193.102, 172.217.193.101, ...\n",
            "Connecting to docs.google.com (docs.google.com)|172.217.193.138|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://doc-14-04-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/j8g6m1vsaeoo78t6jb3pbmb06tnl06vc/1581530400000/08752484438609855375/*/1OVRo37agn02mc6yp5p6-wtJ8Hyb-YMXR?e=download [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2020-02-12 18:08:44--  https://doc-14-04-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/j8g6m1vsaeoo78t6jb3pbmb06tnl06vc/1581530400000/08752484438609855375/*/1OVRo37agn02mc6yp5p6-wtJ8Hyb-YMXR?e=download\n",
            "Resolving doc-14-04-docs.googleusercontent.com (doc-14-04-docs.googleusercontent.com)... 74.125.141.132, 2607:f8b0:400c:c06::84\n",
            "Connecting to doc-14-04-docs.googleusercontent.com (doc-14-04-docs.googleusercontent.com)|74.125.141.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 503663 (492K) [text/csv]\n",
            "Saving to: ‘spam.csv’\n",
            "\n",
            "spam.csv            100%[===================>] 491.86K  --.-KB/s    in 0.004s  \n",
            "\n",
            "2020-02-12 18:08:44 (109 MB/s) - ‘spam.csv’ saved [503663/503663]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RcHV1lUwtH-n",
        "colab_type": "code",
        "outputId": "0b2e8076-c4d4-446a-dc95-d6cae6621f03",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!ls"
      ],
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sample_data  spam.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eXVQCF-ovo4G",
        "colab_type": "text"
      },
      "source": [
        "There are two columns: `v1` -- spam or ham indicator, `v2` -- text of the message."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BiKE89v0zMiY",
        "colab_type": "code",
        "outputId": "5ac219dd-a93a-487f-a424-a4b7279a098c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        }
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "df = pd.read_csv(\"spam.csv\", usecols=[\"v1\", \"v2\"], encoding='latin-1')\n",
        "# 1 - spam, 0 - ham\n",
        "df.v1 = (df.v1 == \"spam\").astype(\"int\")\n",
        "df.head()"
      ],
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>v1</th>\n",
              "      <th>v2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>Ok lar... Joking wif u oni...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>U dun say so early hor... U c already then say...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   v1                                                 v2\n",
              "0   0  Go until jurong point, crazy.. Available only ...\n",
              "1   0                      Ok lar... Joking wif u oni...\n",
              "2   1  Free entry in 2 a wkly comp to win FA Cup fina...\n",
              "3   0  U dun say so early hor... U c already then say...\n",
              "4   0  Nah I don't think he goes to usf, he lives aro..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 140
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nXQhTzrCv-Nk",
        "colab_type": "text"
      },
      "source": [
        "Your task is to split the data to train/dev/test. Make sure that each row appears only in one of the splits."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ga5Qydpw-gdQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 0.15 for val, 0.15 for test, 0.7 for train\n",
        "val_size = int(df.shape[0] * 0.15)\n",
        "test_size = int(df.shape[0] * 0.15)\n",
        "\n",
        "\"\"\"\n",
        "YOUR CODE GOES HERE\n",
        "\"\"\"\n",
        "\n",
        "train_texts, train_labels = df[\"v2\"].iloc[:70], df[\"v1\"].iloc[:70]\n",
        "val_texts, val_labels = df[\"v2\"].iloc[70:70 + val_size], df[\"v1\"].iloc[70:70 + val_size]\n",
        "test_texts, test_labels   = df[\"v2\"].iloc[70 + val_size:val_size + test_size], df[\"v1\"].iloc[70 + val_size:val_size + test_size]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QGyHG4lBISP2",
        "colab_type": "text"
      },
      "source": [
        "## Data Processing\n",
        "\n",
        "The task is to create bag-of-words features: tokenize the text, index each token, represent the sentence as a dictionary of tokens and their counts, limit the vocabulary to $n$ most frequent tokens. In the lab we use built-in `sklearn` function, `sklearn.feature_extraction.text.CountVectorizer`. \n",
        "**In this HW, you are required to implement the `Vectorizer` on your own without using `sklearn` built-in functions.**\n",
        "\n",
        "Function `preprocess_data` takes the list of texts and returns list of (lists of tokens). \n",
        "You may use [spacy](https://spacy.io/) or [nltk](https://www.nltk.org/) text processing libraries in `preprocess_data` function. \n",
        "\n",
        "Class `Vectorizer` is used to vectorize the text and to create a matrix of features.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "793EFaQYhHeR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import nltk from nltk.corpus \n",
        "# import stopwords \n",
        "# set(stopwords.words('english'))\n",
        "import spacy as sc\n",
        "def preprocess_data(data):\n",
        "    # This function should return a list of lists of preprocessed tokens for each message\n",
        "    \"\"\"\n",
        "    YOUR CODE GOES HERE\n",
        "    \"\"\"\n",
        "    enModel = sc.load('en_core_web_sm')\n",
        "    stopwords = sc.lang.en.stop_words.STOP_WORDS\n",
        "    # loop through list of sentences and append to list\n",
        "    preprocessed_data = []\n",
        "    for sentence in data:\n",
        "      preprocessed_data.append([token.text for token in enModel(sentence)])\n",
        "    return preprocessed_data\n",
        "# could not remove stop words kept giving me a type error, i think thats why my f1 score is so low\n",
        "train_data = preprocess_data(train_texts)\n",
        "val_data = preprocess_data(val_texts)\n",
        "test_data = preprocess_data(test_texts)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TM2qpOKpjVbD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "class Vectorizer():\n",
        "    def __init__(self, max_features):\n",
        "        self.max_features = max_features\n",
        "        self.vocab_list = None\n",
        "        self.token_to_index = None\n",
        "\n",
        "    def fit(self, dataset):\n",
        "        self.dataset = dataset\n",
        "        # Create a vocab list, self.vocab_list, using the most frequent \"max_features\" tokens\n",
        "        # Create a token indexer, self.token_to_index, that will return index of the token in self.vocab\n",
        "        \"\"\"\n",
        "        YOUR CODE GOES HERE\n",
        "        \"\"\"\n",
        "        vocab = {}\n",
        "        #for each sentence in the data set, looks at the word and appends to dictionary\n",
        "        #with the value count of how many times it appeared in the sentence\n",
        "        for sentence in dataset:\n",
        "          for word in sentence:\n",
        "            if word in vocab.keys():\n",
        "              vocab[word] += 1\n",
        "            else:\n",
        "              vocab[word] = 1\n",
        "        #sorts the dictionary by most frequent words\n",
        "        most_frequent = sorted(vocab, key = vocab.get, reverse = True)\n",
        "        # Create a vocab list, self.vocab_list, using the most frequent \"max_features\" tokens\n",
        "        self.vocab_list = most_frequent[:self.max_features]\n",
        "        # Create a token indexer, self.token_to_index, that will return index of the token in self.vocab\n",
        "        i = 0\n",
        "        self.token_to_index = {}\n",
        "        for item in self.vocab_list:\n",
        "          self.token_to_index[item] = i\n",
        "          i += 1\n",
        "        pass\n",
        "\n",
        "    def transform(self, dataset):\n",
        "        # This function transforms text dataset into a matrix, data_matrix\n",
        "        \"\"\"\n",
        "        YOUR CODE GOES HERE\n",
        "        \"\"\"\n",
        "        data_matrix = np.zeros((len(dataset), len(self.vocab_list)))\n",
        "        i = 0\n",
        "        for lists in dataset:\n",
        "          for word in self.vocab_list:\n",
        "            if word in lists:\n",
        "              #fills data matrix with the count of the words in the vocab list that are in the dataset\n",
        "              data_matrix[i][self.token_to_index[word]] = lists.count(word)\n",
        "          i += 1\n",
        "        return data_matrix"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wXMrZXlZjcH7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_features = 2000 # TODO: Replace None with a number\n",
        "vectorizer = Vectorizer(max_features=max_features)\n",
        "vectorizer.fit(train_data)\n",
        "X_train = vectorizer.transform(train_data)\n",
        "X_val = vectorizer.transform(val_data)\n",
        "X_test = vectorizer.transform(test_data)\n",
        "\n",
        "y_train = np.array(train_labels)\n",
        "y_val = np.array(val_labels)\n",
        "y_test = np.array(test_labels)\n",
        "\n",
        "vocab = vectorizer.vocab_list"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cGLg6udky1zo",
        "colab_type": "text"
      },
      "source": [
        "You can add more features to the feature matrix."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s80GgEm6F5DG",
        "colab_type": "code",
        "outputId": "625c355d-1d51-4487-ebf1-686eaa7d0d4d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "\"\"\"\n",
        "YOUR CODE GOES HERE\n",
        "\"\"\"\n",
        "\"\"\" Not sure if this was optional or not\"\"\""
      ],
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' Not sure if this was optional or not'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 129
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wtm7a6JWu9-3",
        "colab_type": "text"
      },
      "source": [
        "## Model\n",
        "\n",
        "We train logistic regression model and save prediction for train, val and test.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wq9stSAbAIZe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Define Logistic Regression model\n",
        "model = LogisticRegression(random_state=0, solver='liblinear')\n",
        "\n",
        "# Fit the model to training data\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make prediction using the trained model\n",
        "y_train_pred = model.predict(X_train)\n",
        "y_val_pred = model.predict(X_val)\n",
        "y_test_pred = model.predict(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3j-Abw7JOqD_",
        "colab_type": "text"
      },
      "source": [
        "## Performance of the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Akg9LvP5DGE8",
        "colab_type": "text"
      },
      "source": [
        "Your task is to report train, val, test accuracies and F1 scores.\n",
        "**You are required to implement `accuracy_score` and `f1_score` methods without using built-in python functions.**\n",
        "\n",
        "Your model should achieve at least **0.95** test accuracy and **0.90** test F1 score."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "chqVbKH6kZyY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def accuracy_score(y_true, y_pred): \n",
        "    # Calculate accuracy of the model's prediction\n",
        "    \"\"\"\n",
        "    YOUR CODE GOES HERE\n",
        "    \"\"\"\n",
        "    accurate = 0\n",
        "    for i in range(len(y_true)):\n",
        "      if y_true[i] == y_pred[i]:\n",
        "        accurate += 1\n",
        "    accuracy = accurate / len(y_true)\n",
        "    return accuracy\n",
        "\n",
        "def f1_score(y_true, y_pred): \n",
        "    # Calculate F1 score of the model's prediction\n",
        "    #f1 = 2*((precision*recall)/(precision + recall))\n",
        "    #precision = tp / tp+fp\n",
        "    #recall = tp / tp + fn\n",
        "    \"\"\"\n",
        "    YOUR CODE GOES HERE\n",
        "    \"\"\"\n",
        "    tp = 0\n",
        "    fp = 0\n",
        "    fn = 0\n",
        "    for i in range(len(y_true)):\n",
        "      if y_true[i] == 1 and y_pred[i] == 1:\n",
        "        tp += 1\n",
        "    for i in range(len(y_true)):\n",
        "      if y_true[i] == 0 and y_pred[i] == 1:\n",
        "        fp += 1\n",
        "    for i in range(len(y_true)):\n",
        "      if y_true[i] == 1 and y_pred[i] == 0:\n",
        "        fn += 1\n",
        "    precision = tp / (tp + fp)\n",
        "    recall = tp / (tp + fn)\n",
        "\n",
        "    f1 = 2*((precision*recall)/(precision + recall))\n",
        "    return f1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MqrMw0udDD04",
        "colab_type": "code",
        "outputId": "2a138d88-635b-4bfb-c206-ca4b497fb423",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "print(f\"Training accuracy: {accuracy_score(y_train, y_train_pred):.3f}, \"\n",
        "      f\"F1 score: {f1_score(y_train, y_train_pred):.3f}\")\n",
        "print(f\"Validation accuracy: {accuracy_score(y_val, y_val_pred):.3f}, \"\n",
        "      f\"F1 score: {f1_score(y_val, y_val_pred):.3f}\")\n",
        "print(f\"Test accuracy: {accuracy_score(y_test, y_test_pred):.3f}, \"\n",
        "      f\"F1 score: {f1_score(y_test, y_test_pred):.3f}\")"
      ],
      "execution_count": 148,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training accuracy: 1.000, F1 score: 1.000\n",
            "Validation accuracy: 0.886, F1 score: 0.469\n",
            "Test accuracy: 0.915, F1 score: 0.532\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FW7P84giGgP4",
        "colab_type": "text"
      },
      "source": [
        "**Question.**\n",
        "Is accuracy the metric that logistic regression optimizes while training? If no, which metric is optimized in logistic regression?\n",
        "\n",
        "**Your answer:** \n",
        "EXTRA CREDIT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ak0h71krLPqX",
        "colab_type": "text"
      },
      "source": [
        "**Question.**\n",
        "In general, does having 0.99 accuracy on test means that the model is great? If no, can you give an example of a case when the accuracy is high but the model is not good? (Hint: why do we use F1 score?)\n",
        "\n",
        "**Your answer:** \n",
        "having 0.99 accuracy does not mean that the model is great, because the accuracy is only the ratio of accurate predictions over all predictions. So if the data isnt balanced, and doesnt take into account false positives and false negatives, it can lead to a false representation of the truth. Like when one is testing for a rare disease."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L_RDI0qdOxwM",
        "colab_type": "text"
      },
      "source": [
        "### Exploration of predicitons"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DHR2OqYCDOxs",
        "colab_type": "text"
      },
      "source": [
        "Show a few examples with true+predicted labels on the train and val sets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5yv8GD-UGXvR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 732
        },
        "outputId": "940de303-99c4-42b8-a19f-8c60b9646ef1"
      },
      "source": [
        "\"\"\"\n",
        "YOUR CODE GOES HERE\n",
        "\"\"\"\n",
        "# 1 - spam, 0 - ham\n",
        "\n",
        "for i in range(5,10):\n",
        "  print(\"Train Set\")\n",
        "  print(train_data[i])\n",
        "  print(\"True Label\", y_train[i])\n",
        "  print(\"Pred Label\", y_train_pred[i])\n",
        "  print(\"Val Set\")\n",
        "  print(val_data[i])\n",
        "  print(\"True Label\", y_val[i])\n",
        "  print(\"Pred Label\", y_val_pred[i])\n"
      ],
      "execution_count": 149,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Set\n",
            "['FreeMsg', 'Hey', 'there', 'darling', 'it', \"'s\", 'been', '3', 'week', \"'s\", 'now', 'and', 'no', 'word', 'back', '!', 'I', \"'d\", 'like', 'some', 'fun', 'you', 'up', 'for', 'it', 'still', '?', 'Tb', 'ok', '!', 'XxX', 'std', 'chgs', 'to', 'send', ',', 'å£1.50', 'to', 'rcv']\n",
            "True Label 1\n",
            "Pred Label 1\n",
            "Val Set\n",
            "['I', 'am', 'waiting', 'machan', '.', 'Call', 'me', 'once', 'you', 'free', '.']\n",
            "True Label 0\n",
            "Pred Label 0\n",
            "Train Set\n",
            "['Even', 'my', 'brother', 'is', 'not', 'like', 'to', 'speak', 'with', 'me', '.', 'They', 'treat', 'me', 'like', 'aids', 'patent', '.']\n",
            "True Label 0\n",
            "Pred Label 0\n",
            "Val Set\n",
            "['That', 's', 'cool', '.', 'i', 'am', 'a', 'gentleman', 'and', 'will', 'treat', 'you', 'with', 'dignity', 'and', 'respect', '.']\n",
            "True Label 0\n",
            "Pred Label 0\n",
            "Train Set\n",
            "['As', 'per', 'your', 'request', \"'\", 'Melle', 'Melle', '(', 'Oru', 'Minnaminunginte', 'Nurungu', 'Vettam', ')', \"'\", 'has', 'been', 'set', 'as', 'your', 'callertune', 'for', 'all', 'Callers', '.', 'Press', '*', '9', 'to', 'copy', 'your', 'friends', 'Callertune']\n",
            "True Label 0\n",
            "Pred Label 0\n",
            "Val Set\n",
            "['I', 'like', 'you', 'peoples', 'very', 'much', ':', ')', 'but', 'am', 'very', 'shy', 'pa', '.']\n",
            "True Label 0\n",
            "Pred Label 0\n",
            "Train Set\n",
            "['WINNER', '!', '!', 'As', 'a', 'valued', 'network', 'customer', 'you', 'have', 'been', 'selected', 'to', 'receivea', 'å£900', 'prize', 'reward', '!', 'To', 'claim', 'call', '09061701461', '.', 'Claim', 'code', 'KL341', '.', 'Valid', '12', 'hours', 'only', '.']\n",
            "True Label 1\n",
            "Pred Label 1\n",
            "Val Set\n",
            "['Does', 'not', 'operate', 'after', ' ', '&', 'lt;#&gt', ';', ' ', 'or', 'what']\n",
            "True Label 0\n",
            "Pred Label 0\n",
            "Train Set\n",
            "['Had', 'your', 'mobile', '11', 'months', 'or', 'more', '?', 'U', 'R', 'entitled', 'to', 'Update', 'to', 'the', 'latest', 'colour', 'mobiles', 'with', 'camera', 'for', 'Free', '!', 'Call', 'The', 'Mobile', 'Update', 'Co', 'FREE', 'on', '08002986030']\n",
            "True Label 1\n",
            "Pred Label 1\n",
            "Val Set\n",
            "['Its', 'not', 'the', 'same', 'here', '.', 'Still', 'looking', 'for', 'a', 'job', '.', 'How', 'much', 'do', 'Ta', \"'s\", 'earn', 'there', '.']\n",
            "True Label 0\n",
            "Pred Label 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "neMQ4VR9GVL3",
        "colab_type": "text"
      },
      "source": [
        "**Question** Print 10 examples from val set which were labeled incorrectly by the model. Why do you think the model got them wrong?\n",
        "\n",
        "**Your answer:** \n",
        "A lot of these speak about cash rewards or promoting an item, which although to us it is obvious that it is spam, the model might not have been able to distinguish"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ssK0jRxGY3u",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "de5bff7c-e140-4da6-ba42-9a2d61677503"
      },
      "source": [
        "\"\"\"\n",
        "YOUR CODE GOES HERE\n",
        "\"\"\"\n",
        "x = 0\n",
        "#for i in range(0,10): dont know why this didn't work\n",
        "for i in range(len(val_data)):\n",
        "  if x != 10:\n",
        "    if y_val[i] != y_val_pred[i]:\n",
        "      x += 1\n",
        "      print(val_data[i])"
      ],
      "execution_count": 155,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Please', 'call', 'our', 'customer', 'service', 'representative', 'on', '0800', '169', '6031', 'between', '10am-9pm', 'as', 'you', 'have', 'WON', 'a', 'guaranteed', 'å£1000', 'cash', 'or', 'å£5000', 'prize', '!']\n",
            "['GENT', '!', 'We', 'are', 'trying', 'to', 'contact', 'you', '.', 'Last', 'weekends', 'draw', 'shows', 'that', 'you', 'won', 'a', 'å£1000', 'prize', 'GUARANTEED', '.', 'Call', '09064012160', '.', 'Claim', 'Code', 'K52', '.', 'Valid', '12hrs', 'only', '.', '150ppm']\n",
            "['You', 'are', 'a', 'winner', 'U', 'have', 'been', 'specially', 'selected', '2', 'receive', 'å£1000', 'or', 'a', '4', '*', 'holiday', '(', 'flights', 'inc', ')', 'speak', 'to', 'a', 'live', 'operator', '2', 'claim', '0871277810910p', '/', 'min', '(', '18', '+', ')']\n",
            "['Todays', 'Voda', 'numbers', 'ending', '7548', 'are', 'selected', 'to', 'receive', 'a', '$', '350', 'award', '.', 'If', 'you', 'have', 'a', 'match', 'please', 'call', '08712300220', 'quoting', 'claim', 'code', '4041', 'standard', 'rates', 'app']\n",
            "['Sunshine', 'Quiz', 'Wkly', 'Q', '!', 'Win', 'a', 'top', 'Sony', 'DVD', 'player', 'if', 'u', 'know', 'which', 'country', 'the', 'Algarve', 'is', 'in', '?', 'Txt', 'ansr', 'to', '82277', '.', 'å£1.50', 'SP', ':', 'Tyrone']\n",
            "['Want', '2', 'get', 'laid', 'tonight', '?', 'Want', 'real', 'Dogging', 'locations', 'sent', 'direct', '2', 'ur', 'mob', '?', 'Join', 'the', 'UK', \"'s\", 'largest', 'Dogging', 'Network', 'bt', 'Txting', 'GRAVEL', 'to', '69888', '!', 'Nt', '.', 'ec2a', '.', '31p.msg@150p']\n",
            "['You', \"'ll\", 'not', 'rcv', 'any', 'more', 'msgs', 'from', 'the', 'chat', 'svc', '.', 'For', 'FREE', 'Hardcore', 'services', 'text', 'GO', 'to', ':', '69988', 'If', 'u', 'get', 'nothing', 'u', 'must', 'Age', 'Verify', 'with', 'yr', 'network', '&', 'try', 'again']\n",
            "['FreeMsg', 'Why', 'have', \"n't\", 'you', 'replied', 'to', 'my', 'text', '?', 'I', \"'m\", 'Randy', ',', 'sexy', ',', 'female', 'and', 'live', 'local', '.', 'Luv', 'to', 'hear', 'from', 'u.', 'Netcollex', 'Ltd', '08700621170150p', 'per', 'msg', 'reply', 'Stop', 'to', 'end']\n",
            "['Customer', 'service', 'annoncement', '.', 'You', 'have', 'a', 'New', 'Years', 'delivery', 'waiting', 'for', 'you', '.', 'Please', 'call', '07046744435', 'now', 'to', 'arrange', 'delivery']\n",
            "['You', 'are', 'a', 'winner', 'U', 'have', 'been', 'specially', 'selected', '2', 'receive', 'å£1000', 'cash', 'or', 'a', '4', '*', 'holiday', '(', 'flights', 'inc', ')', 'speak', 'to', 'a', 'live', 'operator', '2', 'claim', '0871277810810']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ja1hoUIFp_C2",
        "colab_type": "text"
      },
      "source": [
        "## End of Part 1.\n"
      ]
    }
  ]
}